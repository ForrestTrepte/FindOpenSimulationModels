{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FindOpenSimulationModels\n",
    "\n",
    "An experiment to find simulation models such as FMU and Modelica files on the open internet. I am curious how prevalent they are and whether they have inputs and outputs that would be suitable for reinforcement learning environments."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FMUs on GitHub\n",
    "\n",
    "Let's start by looking at FMU files that exist in GitHub repositories.\n",
    "\n",
    "### Manual search\n",
    "\n",
    "We can enter `extension:fmu` in the GitHub search box and choose `All GitHub', resulting in the query https://github.com/search?q=extension%3Afmu&type=code. This resulted in 10,841 code results. That's good that there are thousands of FMU files out there! However, we would need to hit the *Next* button to page through a few files at a time and manually copy their URLs from the web page.\n",
    "\n",
    "### GitHub API search\n",
    "\n",
    "Next, let's try doing this programmatically with the [GitHub search API](https://docs.github.com/en/rest/search). Unfortunately, this doesn't seem to be possible based on this [Reddit](https://www.reddit.com/r/github/comments/dr19uu/finding_all_files_with_a_certain_extension/) and [Stack Overflow](https://stackoverflow.com/questions/58673751/find-all-files-with-certain-filetype-on-github) discussion from three years ago. My results were the same.\n",
    "\n",
    "Here's what I tried (TLDR it didn't work):\n",
    "\n",
    "A **repository** search of `https://api.github.com/search/repositories?q=extension:fmu` only returns one result. It did indeed find a [repository](https://github.com/INTO-CPS-Association/distributed-maestro-fmu) that contains a [singlewatertank-20sim.fmu](https://github.com/INTO-CPS-Association/distributed-maestro-fmu/blob/95922d63eb50c17609320c180319f23d17173c7f/bundle/src/test/resources/singlewatertank-20sim.fmu) file. But there should be many more repositories. It seems that the extensions qualifier is doing something but--if it works--it is only scanning a small subset of GitHub.\n",
    "\n",
    "The [repository API doc](https://docs.github.com/en/rest/search?apiVersion=2022-11-28#search-repositories) says to see [Searching for repositories](https://docs.github.com/en/search-github/searching-on-github/searching-for-repositories) for a detailed list of qualifiers. In that documentation, [Search based on the contents of a repository](https://docs.github.com/en/search-github/searching-on-github/searching-for-repositories#search-based-on-the-contents-of-a-repository) states:\n",
    "\n",
    "> Besides using in:readme, it's not possible to find repositories by searching for specific content within the repository. To search for a specific file or content within a repository, you can use the file finder or code-specific search qualifiers.\n",
    "\n",
    "A **code** search of `https://api.github.com/search/code?q=extension:fmu` returns a `Validation Failed` error with `Must include at least one user, organization, or repository`. So it seems it is not possible to search all of GitHub in this way. In the documentation, [Considerations for code search](https://docs.github.com/en/rest/search?apiVersion=2022-11-28#considerations-for-code-search) states:\n",
    "\n",
    "> * Only files smaller than 384 KB are searchable.\n",
    "> * ...\n",
    "> * You must always include at least one search term when searching source code. For example, searching for language:go is not valid, while amazing language:go is.\n",
    "\n",
    "This will not work for FMU files because we want all of them (not just FMUs containing some particular search term), they are larger than 384 KB, and they are in a binary (zip) format that wouldn't work with a text search.\n",
    "\n",
    "In the [Reddit thread](https://www.reddit.com/r/github/comments/dr19uu/comment/f6ezx4e/?utm_source=share&utm_medium=web2x&context=3) OP Gasp0de also looked into using [GH Archive](https://www.gharchive.org/), but it it doesn't look like GH Archive includes an event type with file information about the contents of repositories/commits.\n",
    "\n",
    "### Scraping\n",
    "\n",
    "Based on the [Information Usage Restrictions](https://docs.github.com/en/site-policy/acceptable-use-policies/github-acceptable-use-policies#7-information-usage-restrictions), it appears that web scraping of GitHub is permitted:\n",
    "\n",
    "> You may use information from our Service for the following reasons, regardless of whether the information was scraped, collected through our API, or obtained otherwise:\n",
    "> \n",
    "> Researchers may use public, non-personal information from the Service for research purposes, only if any publications resulting from that research are open access.\n",
    "Archivists may use public information from the Service for archival purposes.\n",
    "\n",
    "We are researching the nature of FMU files that are present on GitHub with the intent to crate an archive or index of them, so that seems to fit. We don't expect it will be a tremendous amount of data and we won't be spamming anyone. Let's give it a try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting: https://github.com/search?q=extension%3Afmu&type=code\n",
      "Result: status code = 200, url = https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fsearch%3Fq%3Dextension%253Afmu%26type%3Dcode\n",
      "  <title>Sign in to GitHub Â· GitHub</title>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "search_url = 'https://github.com/search?q=extension%3Afmu&type=code'\n",
    "print(f'Getting: {search_url}')\n",
    "search = requests.get(search_url)\n",
    "print(f'Result: status code = {search.status_code}, url = {search.url}')\n",
    "# print response lines containing the string <title>\n",
    "for line in search.text.splitlines():\n",
    "    if '<title>' in line:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening: https://github.com/search?q=extension:fmu&type=code\n",
      "Use the web browser window to log in to GitHub...\n",
      "First page of search results loaded\n",
      "10,841 code results\n",
      "Found 100 pages of results\n"
     ]
    }
   ],
   "source": [
    "# It is requiring us to log in, so we won't just be able to get the results by fetching URLs. Let's try scraping the page with selenium.\n",
    "\n",
    "# While developing, limit the amount of data that is downloaded.\n",
    "# Set to False when ready to download all the data.\n",
    "is_testing = False\n",
    "\n",
    "import time\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "from collections import namedtuple\n",
    "from sortedcontainers import SortedSet\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import urllib.parse\n",
    "\n",
    "# Create a new instance of the Chrome browser\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Navigate to the GitHub website\n",
    "print(f'Opening: {urllib.parse.unquote(search_url)}')\n",
    "driver.get(search_url)\n",
    "\n",
    "# It might be a good idea to automate the user login (if that's possible), but for now do it manually\n",
    "print('Use the web browser window to log in to GitHub...')\n",
    "\n",
    "# Wait for the URL to change to the search page\n",
    "WebDriverWait(driver, 180).until(EC.url_to_be(search_url))\n",
    "\n",
    "print('First page of search results loaded')\n",
    "\n",
    "# Find the heading containing the count of all search results\n",
    "h3_elements = driver.find_elements(By.CSS_SELECTOR, 'h3')\n",
    "for h3_element in h3_elements:\n",
    "    if 'results' in h3_element.text:\n",
    "        print(h3_element.text)\n",
    "\n",
    "# Get number of pages of results\n",
    "current_em = driver.find_element(By.CSS_SELECTOR, 'em.current')\n",
    "page_count = int(current_em.get_attribute('data-total-pages'))\n",
    "print(f'Found {page_count} pages of results')\n",
    "if is_testing:\n",
    "    print(f'Limiting to 3 pages for testing purposes')\n",
    "    page_count = min(page_count, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1/100 order 0/3: https://github.com/search?o=asc&p=1&q=extension:fmu&s=indexed&type=Code\n",
      "This scan has found 0 new FMUs, 0 already known FMUs\n",
      "The entire collection now has 2548 FMUs\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 112\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[39m# Avoid hitting GitHub with too many rapid-fire requests.\u001b[39;00m\n\u001b[0;32m    108\u001b[0m         \u001b[39m# GitHub defines rate limits for the API, such as 10 requests per minute for unauthenticated requests.\u001b[39;00m\n\u001b[0;32m    109\u001b[0m         \u001b[39m# But https://api.github.com/rate_limit doesn't seem to be affected by scraping the web site and it\u001b[39;00m\n\u001b[0;32m    110\u001b[0m         \u001b[39m# isn't clear how rate limits are handled. Let's be cautious.\u001b[39;00m\n\u001b[0;32m    111\u001b[0m         sleep_time \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m \u001b[39mif\u001b[39;00m is_testing \u001b[39melse\u001b[39;00m \u001b[39m20\u001b[39m \u001b[39m# quick results when testing, 20 seconds when downloading all the data (seems like would be reasonable for a human to read each page in this amount of time)\u001b[39;00m\n\u001b[1;32m--> 112\u001b[0m         time\u001b[39m.\u001b[39;49msleep(sleep_time) \u001b[39m# 20 seconds would be plenty of time for a human to read each page of the search results\u001b[39;00m\n\u001b[0;32m    114\u001b[0m result_store\u001b[39m.\u001b[39msave()\n\u001b[0;32m    116\u001b[0m clear_output(wait\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create file to hold results\n",
    "class ResultStore:\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.results = SortedSet()\n",
    "        if os.path.exists(self.filename):\n",
    "            with open(self.filename, 'r') as f:\n",
    "                for line in f:\n",
    "                    self.results.add(line.strip())\n",
    "        self.new_results = 0\n",
    "        self.preexisting_results = 0\n",
    "\n",
    "    def add_result(self, result):\n",
    "        if result in self.results:\n",
    "            self.preexisting_results += 1\n",
    "            return\n",
    "        self.results.add(result)\n",
    "        self.new_results += 1\n",
    "\n",
    "    def print_stats(self):\n",
    "        print(f'This scan has found {self.new_results} new FMUs, {self.preexisting_results} already known FMUs')\n",
    "        print(f'The entire collection now has {len(self.results)} FMUs')\n",
    "\n",
    "    def save(self):\n",
    "        with open(self.filename, 'w') as f:\n",
    "            for result in self.results:\n",
    "                f.write(result + '\\n')\n",
    "\n",
    "result_store = ResultStore('results/github-fmu-search.txt')\n",
    "\n",
    "# Function to process the current page of results\n",
    "def scrape_page_results():\n",
    "    # Get the list items containing the search results (divs with class \"code-list-item\")\n",
    "    item_divs = driver.find_elements(By.CSS_SELECTOR, 'div[class*=\"code-list-item\"]')\n",
    "    for item_div in item_divs:\n",
    "        # Get the link to the FMU file (not the secondary one to the repository)\n",
    "        item_links = item_div.find_elements(By.CSS_SELECTOR, 'a:not(.Link--secondary)')\n",
    "        if (len(item_links) != 1):\n",
    "            print(f'Warning: Parsing problem. Search result item contains {len(item_links)} links, expected just 1 link to the FMU file. Something may have changed on the GitHub website.')\n",
    "        for link in item_links:\n",
    "            result_store.add_result(link.get_attribute(\"href\"))\n",
    "\n",
    "    expected_number_of_results = 10\n",
    "    if len(item_divs) < expected_number_of_results:\n",
    "        print(f'Warning: Search page only has {len(item_divs)} items, expected {expected_number_of_results} items.')\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# It looks like we're limited to 100 pages so, unfortunately, we won't be able to get all the results using this method.\n",
    "# We'll try using different search orders (best/indexed, ascending/descending) to give us different subsets of results.\n",
    "# Best match seems to return results inconsitently. Not sure if ascending/descending has an effect, but we'll try both.\n",
    "max_sort_order = 4\n",
    "def get_search_page_url(page, sort_order, extension='fmu'):\n",
    "    assert 1 <= sort_order <= max_sort_order, f'order must be between 1 and {max_sort_order}'\n",
    "    index = sort_order - 1\n",
    "\n",
    "    order_options = [\n",
    "        'asc', # ascending\n",
    "        'desc', # descending\n",
    "    ]\n",
    "    order = order_options[index % 2]\n",
    "\n",
    "    sort_options = [\n",
    "        'indexed', # recently indexed\n",
    "        '', # best match\n",
    "    ]\n",
    "    sort = sort_options[index // 2]\n",
    "\n",
    "    url = f'https://github.com/search?o={order}&p={page}&q=extension%3A{extension}&s={sort}&type=Code'\n",
    "    return url\n",
    "\n",
    "# Strangely, search pages sometimes fail to load, returning 0 of a small number of results. Retry a few times and track how often this occurs.\n",
    "retry_count = 0\n",
    "max_retry_count = 1 if is_testing else 15\n",
    "PageRetryRecord = namedtuple('PageRetryRecord', ['page', 'retry_count', 'succeeded'])\n",
    "page_retry_data = []\n",
    "\n",
    "# Process each sort order\n",
    "for sort_order in range(1, max_sort_order + 1):\n",
    "    # Process each page of the search results\n",
    "    # Note that we're assuming each search order has the same number of pages. That might not be correct,\n",
    "    # but in practice we always seem to be hitting a limit of 100 pages, so it shouldn't matter.\n",
    "    for current_page in range(1, page_count + 1):\n",
    "        page_url = get_search_page_url(current_page, sort_order)\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        print(f'Scraping page {current_page}/{page_count} order {sort_order}/{max_sort_order}: {urllib.parse.unquote(page_url)}')\n",
    "        result_store.print_stats()\n",
    "\n",
    "        driver.get(page_url)\n",
    "        WebDriverWait(driver, 10).until(EC.url_to_be(page_url))\n",
    "        succeeded = scrape_page_results()\n",
    "        if succeeded or retry_count >= max_retry_count:\n",
    "            # Move on to next page\n",
    "            page_retry_data.append(PageRetryRecord(current_page, retry_count, succeeded))\n",
    "            retry_count = 0\n",
    "\n",
    "            # Save results to file every so often\n",
    "            save_after_number_of_pages = 2 if is_testing else 10\n",
    "            if current_page % save_after_number_of_pages == 0:\n",
    "                result_store.save()\n",
    "        else:\n",
    "            # Failed, repeat this page\n",
    "            retry_count += 1\n",
    "            print(f'Retrying ({retry_count}/{max_retry_count})')\n",
    "\n",
    "        # Avoid hitting GitHub with too many rapid-fire requests.\n",
    "        # GitHub defines rate limits for the API, such as 10 requests per minute for unauthenticated requests.\n",
    "        # But https://api.github.com/rate_limit doesn't seem to be affected by scraping the web site and it\n",
    "        # isn't clear how rate limits are handled. Let's be cautious.\n",
    "        sleep_time = 3 if is_testing else 20 # quick results when testing, 20 seconds when downloading all the data (seems like would be reasonable for a human to read each page in this amount of time)\n",
    "        time.sleep(sleep_time) # 20 seconds would be plenty of time for a human to read each page of the search results\n",
    "\n",
    "result_store.save()\n",
    "\n",
    "clear_output(wait=True)\n",
    "print(f'Done scraping {page_count} pages * {max_sort_order} orders')\n",
    "result_store.print_stats()\n",
    "print()\n",
    "print(\"Retries:\")\n",
    "for i in range(0, max_retry_count+1):\n",
    "    print(f'  succeeded after {i} retries: {sum(1 if x.retry_count == i else 0 for x in page_retry_data)} pages')\n",
    "print(f'  failed: {sum(1 if not x.succeeded else 0 for x in page_retry_data)} pages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://github.com/search?o=asc&p=3&q=extension%3Afmu&s=&type=Code\n",
      "https://github.com/search?o=desc&p=3&q=extension%3Afmu&s=&type=Code\n",
      "https://github.com/search?o=asc&p=3&q=extension%3Afmu&s=indexed&type=Code\n",
      "https://github.com/search?o=desc&p=3&q=extension%3Afmu&s=indexed&type=Code\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "order must be in the range 0-3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m is_testing \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m     27\u001b[0m page_count \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(page_count, \u001b[39m3\u001b[39m)\n\u001b[1;32m---> 28\u001b[0m \u001b[39mprint\u001b[39m(get_search_page_url(\u001b[39m2\u001b[39;49m, \u001b[39m4\u001b[39;49m))\n",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m, in \u001b[0;36mget_search_page_url\u001b[1;34m(page, order, extension)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_search_page_url\u001b[39m(page, order, extension\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfmu\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m----> 5\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39m0\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m order \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m3\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39morder must be in the range 0-3\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m     order_options \u001b[39m=\u001b[39m [\n\u001b[0;32m      8\u001b[0m         \u001b[39m'\u001b[39m\u001b[39masc\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m# ascending\u001b[39;00m\n\u001b[0;32m      9\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mdesc\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m# descending\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     ]\n\u001b[0;32m     11\u001b[0m     o \u001b[39m=\u001b[39m order_options[order \u001b[39m%\u001b[39m \u001b[39m2\u001b[39m]\n",
      "\u001b[1;31mAssertionError\u001b[0m: order must be in the range 0-3"
     ]
    }
   ],
   "source": [
    "# It looks like we're limited to 100 pages so, unfortunately, we won't be able to get all the results using this method.\n",
    "# We'll try using different search orders (order = 0-3 for best/indexed ascending/descending) to give us different subsets of results.\n",
    "# Best match seems to return results inconsitently. Not sure if ascending/descending has an effect, but we'll try both.\n",
    "def get_search_page_url(page, order, extension='fmu'):\n",
    "    assert 0 <= order <= 3, \"order must be in the range 0-3\"\n",
    "\n",
    "    order_options = [\n",
    "        'asc', # ascending\n",
    "        'desc', # descending\n",
    "    ]\n",
    "    o = order_options[order % 2]\n",
    "\n",
    "    sort_options = [\n",
    "        '', # best match\n",
    "        'indexed', # recently indexed\n",
    "\n",
    "    ]\n",
    "    s = sort_options[order // 2]\n",
    "\n",
    "    url = f'https://github.com/search?o={o}&p={page}&q=extension%3A{extension}&s={s}&type=Code'\n",
    "    return url\n",
    "\n",
    "for i in range(0, 4):\n",
    "    print(get_search_page_url(3, i))\n",
    "\n",
    "is_testing = True\n",
    "page_count = min(page_count, 3)\n",
    "print(get_search_page_url(2, 4))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work\n",
    "\n",
    "- Statistics about FMUs (# params/inputs/outputs, OS platforms)\n",
    "- Take a closer looks at a sampling of FMUs to see if they can be understood and controlled to achieve some sort of objective\n",
    "- How to handle versioning of FMU files? The URLs are https://github.com/{username}/{repository}/blob/{commit_hash}/{file_path}. We could eventually have multiple commit hashes for different versions of the same file. Should we keep them all? Or just the latest. Also note that branch name could be used instead of commit_hash to reference the latest version in a branch. Perhaps we should convert the commit hashes to reference the latest version in the default branch?\n",
    "- Other file types (Modelica, MATLAB, ...?)\n",
    "- Internet search (files outside GitHub)\n",
    "- Security note (executable code in FMU files should be treated as untrusted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FindOpenSimulationModels",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8ee1e9661fac7e11d052fa47f33767e1ae2c17c62148570d01dc46b81a3f0767"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
