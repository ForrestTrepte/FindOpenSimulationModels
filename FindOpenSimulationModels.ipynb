{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FindOpenSimulationModels\n",
    "\n",
    "An experiment to find simulation models such as FMU and Modelica files on the open internet. I am curious how prevalent they are and whether they have inputs and outputs that would be suitable for reinforcement learning environments."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FMUs on GitHub\n",
    "\n",
    "Let's start by looking at FMU files that exist in GitHub repositories.\n",
    "\n",
    "### Manual search\n",
    "\n",
    "We can enter `extension:fmu` in the GitHub search box and choose `All GitHub', resulting in the query https://github.com/search?q=extension%3Afmu&type=code. This resulted in 10,841 code results. That's good that there are thousands of FMU files out there! However, we would need to hit the *Next* button to page through a few files at a time and manually copy their URLs from the web page.\n",
    "\n",
    "### GitHub API search\n",
    "\n",
    "Next, let's try doing this programmatically with the [GitHub search API](https://docs.github.com/en/rest/search). Unfortunately, this doesn't seem to be possible based on this [Reddit](https://www.reddit.com/r/github/comments/dr19uu/finding_all_files_with_a_certain_extension/) and [Stack Overflow](https://stackoverflow.com/questions/58673751/find-all-files-with-certain-filetype-on-github) discussion from three years ago. My results were the same.\n",
    "\n",
    "Here's what I tried (TLDR it didn't work):\n",
    "\n",
    "A **repository** search of `https://api.github.com/search/repositories?q=extension:fmu` only returns one result. It did indeed find a [repository](https://github.com/INTO-CPS-Association/distributed-maestro-fmu) that contains a [singlewatertank-20sim.fmu](https://github.com/INTO-CPS-Association/distributed-maestro-fmu/blob/95922d63eb50c17609320c180319f23d17173c7f/bundle/src/test/resources/singlewatertank-20sim.fmu) file. But there should be many more repositories. It seems that the extensions qualifier is doing something but--if it works--it is only scanning a small subset of GitHub.\n",
    "\n",
    "The [repository API doc](https://docs.github.com/en/rest/search?apiVersion=2022-11-28#search-repositories) says to see [Searching for repositories](https://docs.github.com/en/search-github/searching-on-github/searching-for-repositories) for a detailed list of qualifiers. In that documentation, [Search based on the contents of a repository](https://docs.github.com/en/search-github/searching-on-github/searching-for-repositories#search-based-on-the-contents-of-a-repository) states:\n",
    "\n",
    "> Besides using in:readme, it's not possible to find repositories by searching for specific content within the repository. To search for a specific file or content within a repository, you can use the file finder or code-specific search qualifiers.\n",
    "\n",
    "A **code** search of `https://api.github.com/search/code?q=extension:fmu` returns a `Validation Failed` error with `Must include at least one user, organization, or repository`. So it seems it is not possible to search all of GitHub in this way. In the documentation, [Considerations for code search](https://docs.github.com/en/rest/search?apiVersion=2022-11-28#considerations-for-code-search) states:\n",
    "\n",
    "> * Only files smaller than 384 KB are searchable.\n",
    "> * ...\n",
    "> * You must always include at least one search term when searching source code. For example, searching for language:go is not valid, while amazing language:go is.\n",
    "\n",
    "This will not work for FMU files because we want all of them (not just FMUs containing some particular search term), they are larger than 384 KB, and they are in a binary (zip) format that wouldn't work with a text search.\n",
    "\n",
    "In the [Reddit thread](https://www.reddit.com/r/github/comments/dr19uu/comment/f6ezx4e/?utm_source=share&utm_medium=web2x&context=3) OP Gasp0de also looked into using [GH Archive](https://www.gharchive.org/), but it it doesn't look like GH Archive includes an event type with file information about the contents of repositories/commits.\n",
    "\n",
    "### Scraping\n",
    "\n",
    "Based on the [Information Usage Restrictions](https://docs.github.com/en/site-policy/acceptable-use-policies/github-acceptable-use-policies#7-information-usage-restrictions), it appears that web scraping of GitHub is permitted:\n",
    "\n",
    "> You may use information from our Service for the following reasons, regardless of whether the information was scraped, collected through our API, or obtained otherwise:\n",
    "> \n",
    "> Researchers may use public, non-personal information from the Service for research purposes, only if any publications resulting from that research are open access.\n",
    "Archivists may use public information from the Service for archival purposes.\n",
    "\n",
    "We are researching the nature of FMU files that are present on GitHub with the intent to crate an archive or index of them, so that seems to fit. We don't expect it will be a tremendous amount of data and we won't be spamming anyone. Let's give it a try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting: https://github.com/search?q=extension%3Afmu&type=code\n",
      "Result: status code = 200, url = https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fsearch%3Fq%3Dextension%253Afmu%26type%3Dcode\n",
      "  <title>Sign in to GitHub Â· GitHub</title>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "search_url = 'https://github.com/search?q=extension%3Afmu&type=code'\n",
    "print(f'Getting: {search_url}')\n",
    "search = requests.get(search_url)\n",
    "print(f'Result: status code = {search.status_code}, url = {search.url}')\n",
    "# print response lines containing the string <title>\n",
    "for line in search.text.splitlines():\n",
    "    if '<title>' in line:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the web browser window to log in to GitHub...\n",
      "First page of search results loaded\n"
     ]
    }
   ],
   "source": [
    "# It is requiring us to log in, so we won't just be able to get the results by fetching URLs. Let's try scraping the page with selenium.\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import urllib.parse\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "# Create a new instance of the Chrome browser\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Navigate to the GitHub website\n",
    "print(f'Opening: {urllib.parse.unquote(search_url)}')\n",
    "driver.get(search_url)\n",
    "\n",
    "# It might be a good idea to automate the user login (if that's possible), but for now do it manually\n",
    "print('Use the web browser window to log in to GitHub...')\n",
    "\n",
    "# Wait for the URL to change to the search page\n",
    "WebDriverWait(driver, 180).until(EC.url_to_be(search_url))\n",
    "\n",
    "print('First page of search results loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done scraping results: 3/3 pages, 27 FMUs found\n"
     ]
    }
   ],
   "source": [
    "# While developing, limit the amount of data that is downloaded.\n",
    "# Set to False when ready to download all the data.\n",
    "is_testing = True\n",
    "\n",
    "# Get number of pages of results\n",
    "current_em = driver.find_element(By.CSS_SELECTOR, 'em.current')\n",
    "page_count = int(current_em.get_attribute('data-total-pages'))\n",
    "print(f'Found {page_count} pages of results')\n",
    "if is_testing:\n",
    "    print(f'Limiting to 3 pages for testing purposes')\n",
    "    page_count = min(page_count, 3)\n",
    "\n",
    "# Create file to hold results\n",
    "results_file = open('results/github-fmu-search.txt', 'w')\n",
    "fmu_count = 0\n",
    "\n",
    "# Function to process the current page of results\n",
    "def save_page_results():\n",
    "    # Get the list items containing the search results (divs with class \"code-list-item\")\n",
    "    item_divs = driver.find_elements(By.CSS_SELECTOR, 'div[class*=\"code-list-item\"]')\n",
    "    for item_div in item_divs:\n",
    "        # Get the link to the FMU file (not the secondary one to the repository)\n",
    "        item_links = item_div.find_elements(By.CSS_SELECTOR, 'a:not(.Link--secondary)')\n",
    "        if (len(item_links) != 1):\n",
    "            print(f'Warning: Parsing problem. Search result item contains {len(item_links)} links, expected just 1 link to the FMU file. Something may have changed on the GitHub website.')\n",
    "        for link in item_links:\n",
    "            # Write link to the results file\n",
    "            results_file.write(f'{link.get_attribute(\"href\")}\\n')\n",
    "            global fmu_count\n",
    "            fmu_count += 1\n",
    "\n",
    "# Process each page of the search results\n",
    "current_page = 1\n",
    "while current_page <= page_count:\n",
    "    page_url = f'https://github.com/search?p={current_page}&q=extension%3Afmu&type=Code'\n",
    "    clear_output(wait=True)\n",
    "    print(f'Scraping results: {current_page}/{page_count} pages, {fmu_count} FMUs found')\n",
    "    driver.get(page_url)\n",
    "    WebDriverWait(driver, 10).until(EC.url_to_be(page_url))\n",
    "    save_page_results()\n",
    "    current_page += 1\n",
    "\n",
    "    # Wait a bit before going to the next page, to avoid hitting GitHub with too many requests\n",
    "    # TODO: Check rate limit status using https://docs.github.com/en/rest/overview/resources-in-the-rest-api?apiVersion=2022-11-28#rate-limiting\n",
    "    time.sleep(5)\n",
    "\n",
    "clear_output(wait=True)\n",
    "print(f'Done scraping results: {current_page-1}/{page_count} pages, {fmu_count} FMUs found')\n",
    "\n",
    "# Close results file\n",
    "results_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work\n",
    "\n",
    "- Statistics about FMUs (# params/inputs/outputs, OS platforms)\n",
    "- Take a closer looks at a sampling of FMUs to see if they can be understood and controlled to achieve some sort of objective\n",
    "- Modelica files\n",
    "- Internet search (files outside GitHub)\n",
    "- Security note (executable code in FMU files should be treated as untrusted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FindOpenSimulationModels",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8ee1e9661fac7e11d052fa47f33767e1ae2c17c62148570d01dc46b81a3f0767"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
